{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f73817",
   "metadata": {},
   "source": [
    "# Project 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string # contains punctuation to remove\n",
    "from cleantext import clean # contains emojis to remove\n",
    "import re # used for working with string data\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') # Needed for lemmatization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872fea0",
   "metadata": {},
   "source": [
    "The data for this project came from Kaggle. It contains The qualitative ratings and numerical ratings for books on the Goodreads website, as well as other variables (such as the number of votes and comments the rating text recieved). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309057ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data \n",
    "goodreads = pd.read_csv(\"data/goodreads_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70570d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bfe592",
   "metadata": {},
   "source": [
    "### General Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef657abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at how much data we have\n",
    "print(f\"The dataset has {len(goodreads)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93d284",
   "metadata": {},
   "source": [
    "We really don't need that much data for the purpose of this project. We should cut it down. Let's look at other factors before deciding how much to cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive stats for the numerical variables\n",
    "goodreads.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc7ba8",
   "metadata": {},
   "source": [
    "It really doesn't look like there are that many votes or comments, let's drop those and just use the text to make things easier. We also don't need the ids and the dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99330d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads = goodreads[[\"review_text\", \"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4bf8b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# It looks like the ratings are really skewed\n",
    "sns.countplot(x = goodreads.rating, palette = \"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a8668",
   "metadata": {},
   "source": [
    "We want to recode the ratings to a binary variable. Given the skew, let's do the following:\n",
    "- **1 - 3 = 0 (not great)**  \n",
    "- **4 - 5 = 1 (great)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c86dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode rating\n",
    "not_great = [1, 2, 3]\n",
    "great = [4, 5]\n",
    "\n",
    "conditions = [goodreads.rating.isin(not_great), goodreads.rating.isin(great)]\n",
    "values = [0, 1]\n",
    "\n",
    "\n",
    "goodreads[\"rating\"]  = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = goodreads.rating, palette = \"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ad522",
   "metadata": {},
   "source": [
    "This is still a lot of data. The text preprocessing is going to take forever. Let's take a sample of 50,000 of each rating (so 100,000 total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads = goodreads.groupby(\"rating\").sample(50000, random_state = 10).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f596119",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a960d",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a08c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean the review text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Add \"book\" to the stopwords, given the context of this analysis.\n",
    "# Note: This only improved the F1 score by .005 by slightly increasing the recall. \n",
    "# So I opted to leave it out since it will probably be important for bi-grams.\n",
    "# stop_words.add(\"book\") \n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = text.lower() # Make everything lowercase\n",
    "    cleaned_text = \"\".join(char for char in cleaned_text if char not in string.punctuation) # Remove punctuation\n",
    "    cleaned_text = re.sub(r\"http\\S+\", \"\", cleaned_text) # remove URLs from the review\n",
    "    cleaned_text = re.sub(\"\\n\", \"\", cleaned_text) # I noticed multiple reviews had  the new line symbol (see below cell), so remove those\n",
    "    cleaned_text = re.sub('  ', ' ', cleaned_text) # remove extra spaces (if any)\n",
    "    cleaned_text = cleaned_text.split(' ') # Temporarily tokenize for stopword removal and lemmatization\n",
    "    \n",
    "    # The following line of code does a couple things:\n",
    "    ### 1: Removes the word if it is a stopword (e.g., common word that will not be useful, like \"the\")\n",
    "    ### 2: Lemmatizes the word (e.g., convert to base form so similar words are not counted as separate words)\n",
    "    cleaned_text = [lemmatizer.lemmatize(word) for word in cleaned_text if word not in stop_words]\n",
    "    cleaned_text = ' '.join(cleaned_text) # get back to a string\n",
    "       \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e398a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text in each review\n",
    "goodreads.review_text = goodreads.review_text.apply(lambda review: clean_text(review))\n",
    "# Check first 5 reviews\n",
    "goodreads.review_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b91c1",
   "metadata": {},
   "source": [
    "### Train/Test Split and Bag of Words\n",
    "Split the data, create a bag of words from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(goodreads, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictors and outcomes\n",
    "X_train = train.review_text.to_numpy()\n",
    "y_train = train.rating.to_numpy()\n",
    "\n",
    "X_test = test.review_text.to_numpy()\n",
    "y_test = test.rating.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1793f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CV = CountVectorizer(stop_words = None, max_features = 5000) # Only get the top 5000 words\n",
    "CV.fit(X_train)\n",
    "X_train_uni = CV.transform(X_train)\n",
    "# The training data is now a Bag of Words (a matrix of word counts)\n",
    "X_train_uni.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top ten most frequent words in the vocabulary\n",
    "vocabulary = CV.vocabulary_.items() # Get all of the words in the vocabulary\n",
    "totals = X_train_uni.sum(axis = 0) # count words\n",
    "frequencies = [(word, totals[0, index]) for word, index in vocabulary] # Get the frequencies for all the words\n",
    "frequencies = sorted(frequencies, key = lambda x: x[1], reverse  = True) # sort words based on the frequencies\n",
    "frequencies[ : 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c645d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the vocabulary from the training data to transform the tokens in the test set. \n",
    "X_test_uni = CV.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d196e73",
   "metadata": {},
   "source": [
    "### Model fit and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4640e6",
   "metadata": {},
   "source": [
    "'Base' Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1af603",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "print(\"Mean Accuracies:\")\n",
    "scores = cross_val_score(model, X_train_uni, y_train, n_jobs=-1)\n",
    "display(pd.DataFrame(scores))\n",
    "print(\"F1 Scores:\")\n",
    "scores = cross_val_score(model, X_train_uni, y_train, scoring=\"f1\", n_jobs=-1)\n",
    "display(pd.DataFrame(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499e4fd",
   "metadata": {},
   "source": [
    "Hyper-Parameter Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f5162",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"alpha\": [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "model = MultinomialNB()\n",
    "search = GridSearchCV(model, param_grid=param_grid, scoring=\"f1\", n_jobs=-1, return_train_score=True)\n",
    "search.fit(X_train_uni, y_train)\n",
    "\n",
    "# display(pd.DataFrame(search.cv_results_))\n",
    "print(f\"Best f1 Score: {search.best_score_}\")\n",
    "print(f\"Best Estimator: {search.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd2309",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5704947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=100)\n",
    "model.fit(X_train_uni, y_train) # fit best model\n",
    "\n",
    "y_pred = model.predict(X_test_uni) # make predictions\n",
    "\n",
    "# calculate scoring metrics\n",
    "(p, r, f, s) = precision_recall_fscore_support(y_test, y_pred, pos_label=1, labels=[0])\n",
    "print(f\"precision: {p[0]}\\nrecall: {r[0]}\\nfscore: {f[0]}\")\n",
    "\n",
    "# plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['not-great', 'great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9618d",
   "metadata": {},
   "source": [
    "Try using bigrams (e.g., word pairs) instead of just single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51739de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_bi_gram = CountVectorizer(stop_words = None, ngram_range=(2,2), max_features = 10000) # Only get the top 10000 bi-grams\n",
    "CV_bi_gram.fit(X_train)\n",
    "X_train_bi = CV_bi_gram.transform(X_train)\n",
    "# The training data is now a Bag of Words (a matrix of bi-gram counts)\n",
    "X_train_bi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top ten most frequent bi-grams in the vocabulary\n",
    "vocabulary_bi = CV_bi_gram.vocabulary_.items() # Get all of the bi-grams in the vocabulary\n",
    "totals_bi = X_train_bi.sum(axis = 0) # count bi_grams\n",
    "frequencies_bi = [(word, totals_bi[0, index]) for word, index in vocabulary_bi] # Get the frequencies for all the bi-grams\n",
    "frequencies_bi = sorted(frequencies_bi, key = lambda x: x[1], reverse  = True) # sort bi-grams based on the frequencies\n",
    "frequencies_bi[ : 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the vocabulary from the training data to transform the tokens in the test set. \n",
    "X_test_bi = CV_bi_gram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99663fed",
   "metadata": {},
   "source": [
    "### Model fit and accuracy with bi-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1c46b",
   "metadata": {},
   "source": [
    "'Base' Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "print(\"Mean Accuracies (bi-gram):\")\n",
    "scores = cross_val_score(model, X_train_bi, y_train, n_jobs=-1)\n",
    "display(pd.DataFrame(scores))\n",
    "print(\"F1 Scores (bi_gram):\")\n",
    "scores = cross_val_score(model, X_train_bi, y_train, scoring=\"f1\", n_jobs=-1)\n",
    "display(pd.DataFrame(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e974e5",
   "metadata": {},
   "source": [
    "Hyper-Parameter Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"alpha\": [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "model = MultinomialNB()\n",
    "search_bi = GridSearchCV(model, param_grid=param_grid, scoring=\"f1\", n_jobs=-1, return_train_score=True)\n",
    "search_bi.fit(X_train_bi, y_train)\n",
    "\n",
    "# display(pd.DataFrame(search.cv_results_))\n",
    "print(f\"Best f1 Score (bi-gram): {search_bi.best_score_}\")\n",
    "print(f\"Best Estimator (bi_gram): {search_bi.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3daf873",
   "metadata": {},
   "source": [
    "### Final Model for bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeadf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=10)\n",
    "model.fit(X_train_bi, y_train) # fit best model\n",
    "\n",
    "y_pred_bi = model.predict(X_test_bi) # make predictions\n",
    "\n",
    "# calculate scoring metrics\n",
    "(p, r, f, s) = precision_recall_fscore_support(y_test, y_pred_bi, pos_label=1, labels=[0])\n",
    "print(f\"precision: {p[0]}\\nrecall: {r[0]}\\nfscore: {f[0]}\")\n",
    "\n",
    "# plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_bi)\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['not-great', 'great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e155878",
   "metadata": {},
   "source": [
    "Note: Doubling the training data (e.g., from 10000 total to 20000 total) increased the f1 scores by about .04 each. Increasing to 100000 total only increased the scores by about .02."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
